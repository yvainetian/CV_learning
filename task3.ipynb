{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOZFpj_OLZ02",
        "colab_type": "text"
      },
      "source": [
        "# 3. 字符识别模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvP4JxLuLtEU",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 学习目标\n",
        "    * 学习CNN基础和原理\n",
        "    * 使用Pytorch框架构建CNN模型，并完成训练\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_FuzeLMc26",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 CNN介绍\n",
        "    卷积神经网络（简称CNN）是一类特殊的人工神经网络，是深度学习中重要的一个分支。CNN在很多领域都表现优异，精度和速度比传统计算学习算法高很多。特别是在计算机视觉领域，CNN是解决*图像分类、图像检索、物体检测和语义分割*的主流模型。\n",
        "    “卷积” 和 “神经网络”. 卷积也就是说神经网络不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解. \n",
        "    具体来说, 卷积神经网络有一个批量过滤器, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了.\n",
        "    在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担. 也就是说在卷集的时候, 不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CotABv9slryj",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Pytorch构建CNN模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuVS0i60lxlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义模型\n",
        "class SVHN_Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SVHN_Model1, self).__init__()\n",
        "        # CNN提取特征模块\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),\n",
        "            nn.ReLU(),  \n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # \n",
        "        self.fc1 = nn.Linear(32*3*7, 11)\n",
        "        self.fc2 = nn.Linear(32*3*7, 11)\n",
        "        self.fc3 = nn.Linear(32*3*7, 11)\n",
        "        self.fc4 = nn.Linear(32*3*7, 11)\n",
        "        self.fc5 = nn.Linear(32*3*7, 11)\n",
        "        self.fc6 = nn.Linear(32*3*7, 11)\n",
        "    \n",
        "    def forward(self, img):        \n",
        "        feat = self.cnn(img)\n",
        "        feat = feat.view(feat.shape[0], -1)\n",
        "        c1 = self.fc1(feat)\n",
        "        c2 = self.fc2(feat)\n",
        "        c3 = self.fc3(feat)\n",
        "        c4 = self.fc4(feat)\n",
        "        c5 = self.fc5(feat)\n",
        "        c6 = self.fc6(feat)\n",
        "        return c1, c2, c3, c4, c5, c6\n",
        "    \n",
        "#这里需要注意输入输出时大小变换"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5vrcEh5l34j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#训练模型\n",
        "\n",
        "# 损失函数\n",
        "criterion = nn.CrossEntropyLoss() #为什么用这个\n",
        "# 优化器\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.005)\n",
        "\n",
        "loss_plot, c0_plot = [], []\n",
        "# 迭代10个Epoch\n",
        "for epoch in range(10):\n",
        "    for data in train_loader:\n",
        "        c0, c1, c2, c3, c4, c5 = model(data[0])\n",
        "        loss = criterion(c0, data[1][:, 0]) + \\\n",
        "                criterion(c1, data[1][:, 1]) + \\\n",
        "                criterion(c2, data[1][:, 2]) + \\\n",
        "                criterion(c3, data[1][:, 3]) + \\\n",
        "                criterion(c4, data[1][:, 4]) + \\\n",
        "                criterion(c5, data[1][:, 5])\n",
        "        loss /= 6\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_plot.append(loss.item())\n",
        "        c0_plot.append((c0.argmax(1) == data[1][:, 0]).sum().item()*1.0 / c0.shape[0])\n",
        "        \n",
        "    print(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LrJPDM0mHxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#为了追求精度，也可以使用在ImageNet数据集上的预训练模型\n",
        "\n",
        "class SVHN_Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SVHN_Model1, self).__init__()\n",
        "                \n",
        "        model_conv = models.resnet18(pretrained=True)\n",
        "        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model_conv = nn.Sequential(*list(model_conv.children())[:-1])\n",
        "        self.cnn = model_conv\n",
        "        \n",
        "        self.fc1 = nn.Linear(512, 11)\n",
        "        self.fc2 = nn.Linear(512, 11)\n",
        "        self.fc3 = nn.Linear(512, 11)\n",
        "        self.fc4 = nn.Linear(512, 11)\n",
        "        self.fc5 = nn.Linear(512, 11)\n",
        "    \n",
        "    def forward(self, img):        \n",
        "        feat = self.cnn(img)\n",
        "        # print(feat.shape)\n",
        "        feat = feat.view(feat.shape[0], -1)\n",
        "        c1 = self.fc1(feat)\n",
        "        c2 = self.fc2(feat)\n",
        "        c3 = self.fc3(feat)\n",
        "        c4 = self.fc4(feat)\n",
        "        c5 = self.fc5(feat)\n",
        "        return c1, c2, c3, c4, c5"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}